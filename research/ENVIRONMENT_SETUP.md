# CognOS Research Environment Setup

## Rekommenderad Lokal Setup (Ollama)

**F√∂rdelar:**
- ‚úÖ Ingen API-kostnad
- ‚úÖ Reproducerbarhet (samma modell varje g√•ng)
- ‚úÖ Snabbare iteration (lokal inference)
- ‚úÖ Privacy (inget l√§cker ut)
- ‚úÖ Offline-capable

---

## 1. Skapa Python Environment

```bash
cd /media/bjorn/iic/cognos-standalone/research

# Skapa venv (use --copies for mounted filesystems)
python3 -m venv --copies .venv

# Aktivera
source .venv/bin/activate

# Installera dependencies
pip install --upgrade pip
pip install pyyaml numpy requests
```

**Optional (om Groq ska anv√§ndas):**
```bash
pip install groq
```

---

## 2. Kontrollera Ollama Models

Du har redan:
```bash
ollama ls
```

**Output:**
- `qwen2.5:7b` (4.7 GB) ‚Äî **Rekommenderad** f√∂r research (b√§sta reasoning)
- `tinyllama:latest` (637 MB) ‚Äî Snabbt men svagt reasoning
- `phi3:mini` (2.2 GB) ‚Äî Bra balans f√∂r snabba tester
- `mistral-large-3:675b-cloud` ‚Äî Cloud model (kr√§ver API)

---

## 3. Modellval per Experiment

### üî¨ Experiment 001 (Divergence Activation)
**Modell:** `qwen2.5:7b`  
**Varf√∂r:** Beh√∂ver robust reasoning f√∂r att generera divergerande svar

### üî¨ Experiment 002 (Epistemic Gain)
**Modell:** `qwen2.5:7b`  
**Varf√∂r:** Beh√∂ver clarity/actionability i svar (viktigt f√∂r j√§mf√∂relse)

### üî¨ Experiment 003 (Ill-Posed Detection)
**Modell:** `qwen2.5:7b`  
**Varf√∂r:** Beh√∂ver kunna resonera om fr√•gornas kvalitet

**Alternative (f√∂r snabba tester):**
- `phi3:mini` ‚Äî Snabbare men l√§gre kvalitet

---

## 4. Anv√§ndning i Experiment Runners

**Enkel setup:**
```python
from llm_backend import create_ollama_backend

# Skapa backend
llm = create_ollama_backend(model="qwen2.5:7b", temperature=0.7)

# Anv√§nd som vanlig funktion
response = llm.ask(
    system="You are a helpful assistant.",
    prompt="What is 2+2?",
    temperature=0.0
)
```

**Auto-detect (fallback till Groq/Mock):**
```python
from llm_backend import auto_backend

llm = auto_backend(prefer_local=True)
# F√∂rs√∂ker Ollama f√∂rst, sen Groq, sen Mock
```

---

## 5. Test LLM Backend

```bash
cd /media/bjorn/iic/cognos-standalone/research
source .venv/bin/activate

# Testa backends
python llm_backend.py
```

**F√∂rv√§ntat output:**
```
Testing LLM backends...

1. Testing Ollama (qwen2.5:7b):
   Response: 2+2 equals 4.

2. Testing auto-detect:
‚úì Using Ollama (local)
   Response: Hello!
```

---

## 6. Uppdatera Experiment Runners

**Tidigare:**
```python
from groq import Groq
client = Groq()
```

**Nu:**
```python
from llm_backend import create_ollama_backend
llm = create_ollama_backend("qwen2.5:7b")
```

---

## 7. Performance Expectations

**Qwen2.5:7b p√• din laptop:**
- **Tokens/sec:** ~20-40 (beroende p√• GPU)
- **Response time:** 5-15 sekunder per fr√•ga
- **Experiment 001:** ~250 iterations √ó 10s = ~40 minuter totalt

**Phi3:mini (snabbare):**
- **Tokens/sec:** ~40-80
- **Response time:** 2-5 sekunder
- **Experiment 001:** ~250 iterations √ó 3s = ~12 minuter

---

## 8. Configuration per Experiment

**exp_001_divergence/config.yaml:**
```yaml
llm:
  backend: "ollama"
  model: "qwen2.5:7b"
  temperature: 0.7
  base_url: "http://localhost:11434"
```

**exp_002_epistemic_gain/config.yaml:**
```yaml
llm:
  backend: "ollama"
  model: "qwen2.5:7b"
  temperature: 0.7

baseline_llm:
  backend: "ollama"
  model: "qwen2.5:7b"  # Samma modell, olika prompt
  temperature: 0.7
```

---

## 9. Reproducibility Settings

**F√∂r maximal reproducerbarhet:**
```python
llm = create_ollama_backend(
    model="qwen2.5:7b",
    temperature=0.0,  # Deterministisk
)
```

**F√∂r naturlig variation (Monte Carlo sampling):**
```python
llm = create_ollama_backend(
    model="qwen2.5:7b",
    temperature=0.7,  # Default
)
```

---

## 10. Troubleshooting

### Problem: "Connection refused"
```bash
# Starta Ollama service
ollama serve
```

### Problem: "Model not found"
```bash
# Lista tillg√§ngliga modeller
ollama ls

# Ladda ner modell om den saknas
ollama pull qwen2.5:7b
```

### Problem: L√•ngsamma svar
```bash
# Anv√§nd mindre modell f√∂r snabba tester
# √Ñndra i config:
model: "phi3:mini"  # 2.2 GB ist√§llet f√∂r 4.7 GB
```

---

## Rekommendation

**F√∂r produktionsk√∂rningar:**
- Anv√§nd `qwen2.5:7b`
- Temperature 0.7 f√∂r variation
- N=30-50 iterationer per fr√•ga

**F√∂r snabba tester/debugging:**
- Anv√§nd `phi3:mini`
- Temperature 0.0 f√∂r reproducerbarhet
- N=5 iterationer

**F√∂r offline/utan Ollama:**
- Anv√§nd `auto_backend()` ‚Üí fallback till Mock

---

**Next step:** Uppdatera `run_exp_001_divergence.py` att anv√§nda `llm_backend.py`
