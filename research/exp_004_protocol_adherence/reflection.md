# Exp 004 — CognOS Protocol Adherence Across Frontier Models

**Date:** 2026-02-23
**Protocol:** COGNOS MASTER PROTOCOL v1.1
**Test question:** Should a mid-sized hospital deploy an AI diagnostic system with 92% test accuracy but limited validation on diverse populations?
**Models tested:** Lechat (Mistral Large), ChatGPT (GPT-4o), Claude (Sonnet 4.6), Gemini

---

## Scoring Rubric

| Metric | Description |
|---|---|
| pᵢ stated | Did the model explicitly state plausibility estimates for each hypothesis? |
| D formula used | Was D computed as variance(pᵢ) per protocol, or estimated qualitatively? |
| Conf formula correct | Does sigmoid(α·Ē − β·D − γ·U_A − δ·M) match the reported value? |
| Cᵢ included | Was internal coherence reported per hypothesis (required variable)? |
| Decision consistent | Does the reported decision match the Conf threshold gates? |
| Hypotheses ≥ 3 | Protocol requires n ≥ 3 when feasible |

---

## Results

| Metric | Lechat | ChatGPT | Claude | Gemini |
|---|---|---|---|---|
| pᵢ stated | No | No | **Yes** {0.20, 0.40, 0.15, 0.25} | No |
| D formula used | No (0.08 — fabricated) | No (0.35 — qualitative) | Partial (0.006 calculated, adjusted to 0.45) | No (0.65 — qualitative) |
| Conf formula correct | **No** (0.68 vs ~0.54) | N/A (honest: "qualitatively") | **Yes** sigmoid(−0.409) ≈ 0.40 ✓ | No (0.74, formula not shown) |
| Cᵢ included | No | No | No | No |
| Decision consistent | Yes (CAUTION) | Yes (CAUTION) | Yes (CAUTION/DO NOT RELY) | Yes* (CAUTION at 0.74) |
| Hypotheses ≥ 3 | 3 ✓ | 3 ✓ | **4** ✓ | 3 ✓ |

*Gemini's Conf=0.74 is technically CAUTION (< 0.75 threshold), but narrowly. Decision is consistent.

---

## Confidence Verification

### Lechat
Reported: **0.68**
Computed: sigmoid(1.0×0.79 − 1.0×0.08 − 0.8×0.25 − 0.6×0.60) = sigmoid(0.15) ≈ **0.537**
Error: **+0.143** — overconfident relative to formula

### ChatGPT
Reported: **~0.62 (qualitative)**
Computed: Not attempted. Stated explicitly: "Using the protocol's structure qualitatively."
Assessment: Honest non-compliance. No fabrication.

### Claude
Reported: **0.40**
Computed: sigmoid(1.0×0.665 − 1.0×0.45 − 0.8×0.48 − 0.6×0.40) = sigmoid(−0.409) ≈ **0.399**
Error: **≈ 0** — correct ✓

### Gemini
Reported: **0.74**
Ē estimate: mean(0.90, 0.78, 0.40) = 0.693
Computed: sigmoid(0.693 − 0.65 − 0.8×0.28 − 0.6×0.45) = sigmoid(−0.451) ≈ **0.39**
Error: **+0.35** — substantially overconfident relative to formula
Note: pᵢ values not given; Ē estimated from Eᵢ as proxy.

---

## Three Behavioral Patterns

### Pattern A — Confidence Simulation (Lechat, Gemini)
The model follows the output structure and produces plausible-sounding numbers without executing the formula. Numbers are internally consistent with the narrative but not with the mathematics. Gemini additionally produced a hallucinated YouTube video recommendation, indicating tool-call simulation.

### Pattern B — Honest Qualitative Estimation (ChatGPT)
The model explicitly acknowledges it is not computing the formula. Applies the protocol as a reasoning scaffold rather than a calculation procedure. No fabrication — but also no math.

### Pattern C — Formula Execution (Claude)
The only model that stated explicit pᵢ values and showed the sigmoid calculation with correct result. Generated 4 hypotheses instead of 3, expanding hypothesis space as permitted by the protocol. Also added a bias-check section not in the protocol — an extension rather than a deviation.

---

## Universal Omission

**Cᵢ (internal coherence per hypothesis) was omitted by all four models.**

This is a required variable in Section 1 of the protocol. Its universal absence suggests the protocol's variable definition section (Section 1) is not weighted heavily enough in the output structure (Section 11). The output template does not include a dedicated Cᵢ row, making it easy to skip.

**Recommendation:** Add explicit Cᵢ column to the EVIDENCE SCORES table in Section 11 of the protocol.

---

## Research Implication

This experiment distinguishes **two types of CognOS compliance:**

| Type | What it tests |
|---|---|
| Structural adherence | Does the model produce the required sections? |
| Computational adherence | Does the model execute the mathematics correctly? |

All four models showed moderate-to-high structural adherence. Only one (Claude) showed computational adherence. This finding complements Exp 001–003 (code-based CognOS variance testing) by demonstrating that prompt-based CognOS functions as a reliable *reasoning scaffold* but not as a *calculation engine* for most frontier models.

**Connection to epistemic-noise paper:** The variance between confidence values (reported vs. computed) is itself a form of epistemic noise — but generated by the model's text-prediction process rather than its sampling distribution. Lechat's +0.143 error and Gemini's +0.35 error represent systematic overconfidence bias in confidence *reporting*, independent of the model's actual uncertainty.

---

## Next Steps

1. Run same test with Gemini 2.0 Flash vs Gemini 1.5 Pro — does model size change computational adherence?
2. Add Cᵢ explicitly to protocol v1.2 output template
3. Test whether providing a worked example in the protocol improves computation adherence
4. Consider this as Section 4.x in the epistemic-noise paper (prompt-based vs. code-based CognOS)

---

## Addendum — Framing Drift Under Protocol Application

**Date:** 2026-02-23
**Model:** Lechat (Mistral Large)
**Input:** Tweet — "An AI agent that says 'I'm 94% confident' but can't show its work is not an agent — it's a slot machine with a GUI."

### Observation

When asked to evaluate the tweet using the CognOS protocol, Lechat produced structurally valid output (hypotheses, evidence scores, divergence, confidence, decision) with Conf = 0.88, PROCEED.

However, the analysis reframed the input claim silently:

| Level | Statement |
|---|---|
| Input (tweet) | A confidence *value* should be mathematically traceable — calibration |
| Lechat output | AI systems should support interpretability and audit trails — XAI / regulatory compliance |

The evidential sources cited (EU AI Act, XAI literature, HR Summit 2026) are valid support for the *reframed* question — not the original. Lechat's high Conf=0.88 measures the degree of consensus around XAI adoption, not the validity of the calibration argument.

### Classification

This is a fourth behavioral pattern, distinct from the three identified in the main experiment:

| Pattern | Description |
| --- | --- |
| A — Confidence Simulation | Follows structure, produces plausible numbers without executing formula |
| B — Honest Qualitative | Explicitly acknowledges it is not computing; applies protocol as scaffold |
| C — Formula Execution | Executes mathematics correctly |
| **D — Framing Drift** | **Executes protocol on a semantically adjacent question rather than the original** |

Pattern D differs from Pattern A in that the numbers may be locally valid — the error lies upstream, in how the question is parsed before the protocol runs.

### Implication for Protocol Design

A model can achieve structural adherence, produce internally consistent evidence scores, and arrive at a correct decision *for the wrong question*. The protocol has no mechanism to flag when the question under analysis has been silently substituted.

**Potential mitigation:** Add an explicit "Question restatement" field to the protocol output template (Section 11), requiring the model to restate the input claim before analysis. Discrepancy between input and restatement would surface framing drift.

This would also function as a test of semantic fidelity independent of computational adherence.

---

## Addendum 2 — Protocol v1.1 vs v1.2: Lechat Re-test

**Date:** 2026-02-23
**Model:** Lechat (Mistral Large)
**Question:** Same as Exp 004 — hospital AI deployment (92% accuracy, limited diverse validation)
**Protocol versions compared:** v1.1 (original) vs v1.2 (Cᵢ + QUESTION RESTATEMENT added)

### Structural Improvements

| Metric | v1.1 | v1.2 |
| --- | --- | --- |
| QUESTION RESTATEMENT present | No | **Yes** ✓ |
| Cᵢ column in EVIDENCE SCORES | No | **Yes** ✓ |
| Confidence formula shown | No (0.68, no calculation) | Partial: `sigmoid(0.80 − 0.65 − 0.52 − 0.36)` |
| D value | 0.08 (implausibly low) | 0.65 (more realistic) |
| Reported Conf | 0.68 | 0.62 |

### Computational Verification (v1.2)

Protocol values stated: Ē = 0.80, D = 0.65, Q_A = 0.75 → U_A = 0.25, M = 0.60

```text
Correct:   γ·U_A = 0.8 × 0.25 = 0.20
Lechat:    γ·U_A = 0.52          ← fabricated, does not match Q_A = 0.75

Correct:   sigmoid(0.80 − 0.65 − 0.20 − 0.36) = sigmoid(−0.41) ≈ 0.40
Lechat:    sigmoid(0.80 − 0.65 − 0.52 − 0.36) ≈ 0.62
           — two errors: wrong γ·U_A, and sigmoid(−0.73) ≈ 0.33, not 0.62
```

Error: reported Conf 0.62 vs computed ~0.40. The formula is displayed but not executed.

### Assessment

v1.2 successfully improved **structural adherence**: all three new protocol requirements (QUESTION RESTATEMENT, Cᵢ column, formula display) were followed. The D value also became more realistic (0.65 vs 0.08).

However, **computational adherence** did not improve. Lechat remains Pattern A — confidence simulation. The model now shows the formula as a display element without executing the arithmetic. The γ·U_A term was substituted with an inconsistent value (0.52), and the final sigmoid result does not match even the substituted inputs.

### Implication

Protocol v1.2 closes the structural compliance gap identified in Exp 004. It does not close the computational compliance gap. This suggests the two compliance types require different interventions:

| Gap | v1.2 fix | Result |
| --- | --- | --- |
| Structural (missing fields) | Explicit output template rows | Resolved ✓ |
| Computational (wrong math) | "Show your work" instruction | Not resolved |

A worked example embedded in the protocol (Next Step 3 from Exp 004) remains the untested intervention for computational adherence.
